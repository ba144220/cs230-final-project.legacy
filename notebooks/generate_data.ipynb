{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80b0b9d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=os.getenv(\"HF_TOKEN\"))\n",
    "# model = LlamaForCausalLM.from_pretrained(MODEL_NAME, token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b57d14f9-7d54-4a9f-98bd-7e3f631280ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df: dataframe\n",
    "path: str path the store df\n",
    "\"\"\"\n",
    "def to_csv(df, path):\n",
    "    return df.to_csv(path)\n",
    "\n",
    "def to_html(df, path):\n",
    "    return df.to_html(path)\n",
    "\n",
    "def to_tsv(df, path):\n",
    "    return df.to_csv(path, sep='\\t')\n",
    "    \n",
    "\"\"\"\n",
    "df_type: data frame type to be returned any of (csv, html, tsv)\n",
    "task: QA task to be performed any of (arithmetic, item)\n",
    "row_size: row size of the dataset\n",
    "col_size: col size of the dataset\n",
    "file_name: name of file\n",
    "\"\"\"\n",
    "def generate_dataset(df_type, task, row_size, col_size, file_name):\n",
    "    columns = ['Col ' + str(i+1) for i in range(col_size)]\n",
    "    rows = ['Row ' + str(i+1) for i in range(row_size)]\n",
    "    if task == 'arithmetic': \n",
    "        df = pd.DataFrame(np.random.randint(1, 11, size=(row_size, col_size)), columns=columns, index=rows)\n",
    "    elif task == 'item':\n",
    "        df = pd.DataFrame(np.random.choice(list(string.ascii_uppercase), size=(row_size, col_size)), columns=columns, index=rows)\n",
    "    df_type_dict = {'csv': to_csv, 'html': to_html, 'tsv': to_tsv}\n",
    "    path = '../datasets/tables/' + file_name + '.' + df_type \n",
    "    return df_type_dict[df_type](df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d9cbb7-0188-4b97-81ec-eb37c621a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate nxn dim data\n",
    "N_list: list(int) of NxN dim data to generate\n",
    "\"\"\"\n",
    "def generate_n_data_set(N_list):\n",
    "    # 5 samples\n",
    "    # Each with 3 types\n",
    "    # Each with 2 tasks\n",
    "    # Resulting in 30 samples\n",
    "    for n in N_list:\n",
    "        generate_dataset('tsv', 'arithmetic', n, n, 'arithmetic_'+str(n))\n",
    "        generate_dataset('csv', 'arithmetic', n, n, 'arithmetic_'+str(n))\n",
    "        generate_dataset('html', 'arithmetic', n, n, 'arithmetic_'+str(n))\n",
    "    \n",
    "        generate_dataset('tsv', 'item', n, n, 'item_'+str(n))\n",
    "        generate_dataset('csv', 'item', n, n, 'item_'+str(n))\n",
    "        generate_dataset('html', 'item', n, n, 'item_'+str(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a091357-e480-48cd-aeb3-5c3299b35e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic operation functions\n",
    "def get_col_max(df, col_name):\n",
    "    return df[col_name].max()\n",
    "\n",
    "def get_col_min(df, col_name):\n",
    "    return df[col_name].min()\n",
    "\n",
    "def get_col_sum(df, col_name):\n",
    "    return df[col_name].sum()\n",
    "\n",
    "def get_row_max(df, row_num):\n",
    "    return max(df.iloc[row_num].tolist()[1:])\n",
    "\n",
    "def get_row_min(df, row_num):\n",
    "    return min(df.iloc[row_num].values.tolist()[1:])\n",
    "\n",
    "def get_row_sum(df, row_num):\n",
    "    return sum(df.iloc[row_num].values.tolist()[1:])\n",
    "\n",
    "# Item operation functions\n",
    "def get_col_item(df, col_name):\n",
    "    return ','.join(df[col_name].tolist())\n",
    "\n",
    "def get_row_item(df, row_num):\n",
    "    return ','.join(df.iloc[row_num].values.tolist()[1:])\n",
    "    \n",
    "\"\"\"\n",
    "A qa.csv file contain all the questions and their corresponding context and answer\n",
    "question: str (arithmetic operations include fine the max, min, sum)\n",
    "answer: str\n",
    "context: str (only the name of the table)\n",
    "id: unique str\n",
    "task: optional enum (“arithmetic” and “item”)\n",
    "direction: optional enum (“row” or “col”)\n",
    "size: optional tuple[int] \n",
    "\"\"\"\n",
    "ARITHMETIC_OPERATIONS = ['maximum', 'minimum', 'sum']\n",
    "DIRECTIONS = ['row', 'col']\n",
    "\n",
    "def get_question_answer(df, task, direction):\n",
    "    question = \"\"\n",
    "    answer = \"\"\n",
    "    if (task == 'arithmetic'):\n",
    "        # randomly select an operation to perform\n",
    "        operation_ind = np.random.randint(0,len(ARITHMETIC_OPERATIONS)) # 0, 1, 2\n",
    "        operation = ARITHMETIC_OPERATIONS[operation_ind]\n",
    "        col_arithmetic_operation_dict = {'maximum': get_col_max, 'minimum': get_col_min, 'sum': get_col_sum}\n",
    "        row_arithmetic_operation_dict = {'maximum': get_row_max, 'minimum': get_row_min, 'sum': get_row_sum}\n",
    "        \n",
    "        if (direction == 'col'):\n",
    "            col_num = np.random.randint(0, df.shape[1]-1) # col nums are added one more\n",
    "            col_name = \"Col %d\" %(col_num+1)\n",
    "            question = \"What is the %s of %s?\" % (operation, col_name)\n",
    "            answer = col_arithmetic_operation_dict[operation](df, col_name)\n",
    "            \n",
    "        elif (direction == 'row'):\n",
    "            row_num = np.random.randint(0, df.shape[0])   \n",
    "            row_name = \"Row %d\" %(row_num+1)\n",
    "            question = \"What is the %s of %s?\" % (ARITHMETIC_OPERATIONS[operation_ind], row_name)\n",
    "            answer = row_arithmetic_operation_dict[operation](df, row_num)\n",
    "\n",
    "    elif (task == 'item'):\n",
    "        if (direction == 'col'):\n",
    "            col_num = np.random.randint(0, df.shape[1]-1) # col nums are added one more\n",
    "            col_name = \"Col %d\" %(col_num+1)\n",
    "            question = \"List all items in %s\" % (col_name)\n",
    "            answer = get_col_item(df, col_name)\n",
    "            \n",
    "        elif (direction == 'row'):\n",
    "            row_num = np.random.randint(0, df.shape[0])   \n",
    "            row_name = \"Row %d\" %(row_num+1)\n",
    "            question = \"List all items in %s\" % (row_name)\n",
    "            answer = get_row_item(df, row_num)\n",
    "          \n",
    "    return [question, answer]\n",
    "\n",
    "def get_context(file_path):\n",
    "    idx = file_path.find('tables')\n",
    "    return file_path[idx:]\n",
    "\n",
    "def get_id(i):\n",
    "    return \"nt-\" + str(i)\n",
    "\n",
    "def get_task(df):\n",
    "    series = df.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())[1:] # col nums are added one more\n",
    "    if (series.all()):\n",
    "        return 'arithmetic'\n",
    "    return 'item'\n",
    "\n",
    "def get_direction():\n",
    "    #TODO Each with two direction\n",
    "    return \"\"\n",
    "\n",
    "def get_size(df):\n",
    "    reshaped = (df.shape[0], df.shape[1]-1)# col nums are added one more\n",
    "    return str(reshaped)\n",
    "\n",
    "def read_html(path):\n",
    "    #TOOD\n",
    "    table = BeautifulSoup(open(path,'r').read()).find('table')\n",
    "    df = pd.read_html(StringIO(str(table)))[0]\n",
    "    return df\n",
    "    \n",
    "def tables_to_dataset():\n",
    "    path = '../datasets/tables/*'\n",
    "    file_paths = glob.glob(path)\n",
    "    dataset_df = pd.DataFrame(columns=['question', 'answer', 'context', 'id', 'task', 'direction', 'size'])\n",
    "    i = 0\n",
    "    for file_path in file_paths:\n",
    "        # Read file\n",
    "        if (file_path.endswith('.csv')):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif (file_path.endswith('.tsv')):\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "        elif (file_path.endswith('.html')):\n",
    "            df = read_html(file_path)\n",
    "\n",
    "        file_name = file_path.split('/')[-1] \n",
    "        \n",
    "        # Define task\n",
    "        task = get_task(df)\n",
    "        # Generate q, a for all directions\n",
    "        for direction in DIRECTIONS:\n",
    "            q,a = get_question_answer(df, task, direction)\n",
    "            dataset_df.loc[i] = [q, a, get_context(file_path), get_id(i), task, direction, get_size(df)]\n",
    "            i += 1\n",
    "    dataset_path = '../datasets/self_generated_qa.csv'\n",
    "    return dataset_df.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2791235b-522e-405b-be03-e5fd66e594a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "split test train dev into desired percentage\n",
    "train_percent: int\n",
    "test_percent: int\n",
    "val_percent: int\n",
    "\"\"\"\n",
    "def split(train_percent, test_percent, val_percent):\n",
    "    dataset_path = '../datasets/self_generated_qa.csv'\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    probs = np.random.rand(len(df))\n",
    "    training_mask = probs < train_percent \n",
    "    test_mask = (probs>=train_percent) & (probs < test_percent)\n",
    "    validation_mask = probs >= val_percent \n",
    "        \n",
    "    train = df[training_mask]\n",
    "    test = df[test_mask]\n",
    "    validation = df[validation_mask]\n",
    "    \n",
    "    train_path = '../datasets/self_generated_train/qa.csv'\n",
    "    val_path = '../datasets/self_generated_val/qa.csv'\n",
    "    test_path = '../datasets/self_generated_test/qa.csv'\n",
    "    \n",
    "    train.to_csv(train_path, index=False)\n",
    "    validate.to_csv(val_path, index=False)\n",
    "    test.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70caa53-8c14-4b8c-92c7-9cf7dc85662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [4,6,8,10,12]\n",
    "# generate n x n data\n",
    "generate_n_data_set(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c9b79fd-07d9-48c4-994b-1556c50c95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate qa\n",
    "tables_to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b9dcda4-3ca5-4263-8e2e-5e650b51da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train test dev\n",
    "split(0.7, 0.85, 0.85) # 70% train, 15% trest, 15% val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "dde1b786-9dd0-4207-8723-b823ff7d2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIKI table example\n",
    "# TODO: .examples?\n",
    "wiki_path = '../datasets/data/*.tsv'\n",
    "wiki_file_paths = glob.glob(wiki_path)\n",
    "\n",
    "def wiki_tables_to_dataset():\n",
    "    dataset_df = pd.DataFrame(columns=['question', 'answer', 'context', 'id', 'task', 'direction', 'size'])\n",
    "    for i in range(len(wiki_file_paths)):\n",
    "        file_path = wiki_file_paths[i]\n",
    "        df = pd.read_csv(file_path, sep='\\t', on_bad_lines='skip')\n",
    "        dataset_df['question'] = df['utterance']\n",
    "        dataset_df['answer'] = df['targetValue']\n",
    "        dataset_df['context'] = df['context']#.apply(lambda x: x.split('/'[-1:]))\n",
    "        dataset_df['id'] = df['id']\n",
    "    dataset_path = '../datasets/wiki_qa.csv'\n",
    "    return dataset_df.to_csv(dataset_path, index=False)\n",
    "\n",
    "\n",
    "def merge_tables_and_wiki_tables():\n",
    "    dataset_path = '../datasets/qa.csv'\n",
    "    self_generated_path = '../datasets/self_generated_qa.csv'\n",
    "    wiki_dataset_path = \"../datasets/wiki_qa.csv\"\n",
    "    df =  pd.read_csv(self_generated_path)\n",
    "    wiki_df =  pd.read_csv(wiki_dataset_path)\n",
    "    merged_df = pd.concat([df, wiki_df], ignore_index=True)\n",
    "    return merged_df.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e9c869-aa33-4de5-8268-8e795378b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tables_to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2887065f-951c-4e24-851d-1d000c819430",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tables_and_wiki_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
